# -*- coding: utf-8 -*-
"""
Horizon Agent v3.0 - IntegraciÃ³n FastAPI
Agente financiero con distribuciÃ³n correcta de funcionalidades
"""

import os
import sys
import uuid
import mimetypes
from datetime import datetime
from typing import Optional, Dict, List, Any
from dotenv import load_dotenv

from .models import ChatRequest, ChatResponse, ModelType, SessionInfo

# Cargar variables de entorno
load_dotenv()


class HorizonAgent:
    """
    Agente financiero Horizon v3.0 con integraciÃ³n FastAPI
    """
    
    def __init__(self):
        """Inicializa el agente"""
        self.api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY o GOOGLE_API_KEY no configurada")
        
        # Configurar variable de entorno
        if not os.getenv("GEMINI_API_KEY"):
            os.environ["GEMINI_API_KEY"] = self.api_key
        
        # Importar y configurar cliente
        self._setup_client()
        
        # ConfiguraciÃ³n de modelos
        self.model_flash = ModelType.FLASH.value
        self.model_pro = ModelType.PRO.value
        
        # Cache de sesiones
        self.sessions: Dict[str, SessionInfo] = {}
        
        # Prompts del sistema
        self.flash_system_prompt = """
Eres un asistente financiero rÃ¡pido y eficiente especializado en:
- Consultas generales del mercado y definiciones financieras
- BÃºsquedas web de informaciÃ³n actualizada 
- AnÃ¡lisis de contenido de URLs
- ResÃºmenes concisos y respuestas directas
- Noticias financieras en tiempo real

Utiliza las herramientas de bÃºsqueda web y anÃ¡lisis de URLs cuando necesites informaciÃ³n actualizada.
SÃ© directo, preciso y proporciona fuentes cuando sea apropiado.
"""
        
        self.pro_system_prompt = """
Eres un analista financiero cuantitativo senior, escÃ©ptico y riguroso como 
los protagonistas de 'The Big Short'. Tu especialidad es el anÃ¡lisis profundo de:
- Documentos financieros y reportes anuales
- Estados financieros y mÃ©tricas
- GrÃ¡ficos, tablas y datos numÃ©ricos
- Archivos CSV, PDF, imÃ¡genes de documentos

No confÃ­as en opiniones populares; confÃ­as en los datos duros. Identifica patrones, 
riesgos, tendencias y mÃ©tricas clave. Proporciona conclusiones fundamentadas y 
seÃ±ala inconsistencias o riesgos ocultos en los documentos que analices.

IMPORTANTE: Solo trabajas con documentos locales. No tienes acceso a bÃºsquedas web.
"""
    
    def _setup_client(self):
        """Configura el cliente de Google Gemini"""
        try:
            from google import genai
            from google.genai import types
            
            self.genai = genai
            self.types = types
            self.client = genai.Client()
            print("âœ… Cliente Gemini configurado correctamente")
            
        except Exception as e:
            print(f"âŒ Error configurando cliente: {e}")
            raise
    
    def create_session(self) -> str:
        """Crea una nueva sesiÃ³n de chat"""
        session_id = str(uuid.uuid4())
        self.sessions[session_id] = SessionInfo(
            session_id=session_id,
            created_at=datetime.now().isoformat(),
            last_activity=datetime.now().isoformat(),
            message_count=0,
            context={}
        )
        return session_id
    
    def get_session(self, session_id: str) -> Optional[SessionInfo]:
        """Obtiene informaciÃ³n de una sesiÃ³n"""
        return self.sessions.get(session_id)
    
    def update_session(self, session_id: str):
        """Actualiza la actividad de una sesiÃ³n"""
        if session_id in self.sessions:
            session = self.sessions[session_id]
            session.last_activity = datetime.now().isoformat()
            session.message_count += 1
    
    async def process_chat(self, request: ChatRequest) -> ChatResponse:
        """
        Procesa una solicitud de chat y retorna la respuesta
        """
        try:
            # Crear sesiÃ³n si no existe
            if not request.session_id:
                request.session_id = self.create_session()
            elif request.session_id not in self.sessions:
                self.create_session()
            
            # Actualizar sesiÃ³n
            self.update_session(request.session_id)
            
            # Determinar quÃ© modelo usar
            model_to_use = self._determine_model(request)
            
            # Procesar segÃºn el modelo
            if model_to_use == self.model_pro:
                response_data = await self._process_with_pro(request)
            else:
                response_data = await self._process_with_flash(request)
            
            # Crear respuesta
            return ChatResponse(
                response=response_data["text"],
                model_used=response_data["model"],
                tools_used=response_data.get("tools", []),
                metadata=response_data.get("metadata", {}),
                urls_processed=response_data.get("urls", []),
                token_usage=response_data.get("token_usage"),
                session_id=request.session_id
            )
            
        except Exception as e:
            print(f"âŒ Error procesando chat: {e}")
            raise
    
    def _determine_model(self, request: ChatRequest) -> str:
        """Determina quÃ© modelo usar basado en la solicitud"""
        # Si se especifica un modelo, usarlo
        if request.use_model:
            return request.use_model.value
        
        # Si hay contenido de archivo, usar Pro
        if request.file_content:
            return self.model_pro
        
        # Si hay URL pero no archivo, usar Flash
        if request.url:
            return self.model_flash
        
        # Para consultas generales, determinar por palabras clave
        pro_keywords = ['analiza', 'reporte', 'informe', 'mÃ©tricas', 'profundo', 
                       'anÃ¡lisis', 'evaluar', 'documento', 'detallado']
        
        if any(keyword in request.message.lower() for keyword in pro_keywords):
            return self.model_pro
        
        return self.model_flash
    
    async def _process_with_flash(self, request: ChatRequest) -> Dict[str, Any]:
        """Procesa solicitud con Gemini Flash"""
        print(f"âš¡ [Flash]: Procesando consulta")
        
        # Preparar prompt
        prompt_completo = self.flash_system_prompt + "\\n\\nConsulta del usuario: " + request.message
        
        # Agregar URL si estÃ¡ presente
        if request.url:
            prompt_completo += f" {request.url}"
        
        tools_used = []
        urls_processed = []
        
        try:
            # Configurar herramientas
            tools = []
            tools.append(self.types.Tool(google_search=self.types.GoogleSearch()))
            tools_used.append("Google Search")
            
            if request.url:
                tools.append(self.types.Tool(url_context=self.types.UrlContext()))
                tools_used.append("URL Context")
            
            config = self.types.GenerateContentConfig(
                tools=tools,
                response_modalities=["TEXT"]
            )
            
            response = self.client.models.generate_content(
                model=self.model_flash,
                contents=prompt_completo,
                config=config
            )
            
        except Exception as tool_error:
            print(f"âš ï¸ Herramientas no disponibles: {tool_error}")
            # Fallback sin herramientas
            response = self.client.models.generate_content(
                model=self.model_flash,
                contents=prompt_completo
            )
            tools_used = []
        
        # Extraer URLs procesadas si hay URL Context
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'url_context_metadata') and candidate.url_context_metadata:
                for url_meta in candidate.url_context_metadata.url_metadata:
                    urls_processed.append(url_meta.retrieved_url)
        
        # InformaciÃ³n de tokens
        token_usage = None
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            usage = response.usage_metadata
            token_usage = {
                "input_tokens": usage.prompt_token_count,
                "output_tokens": usage.candidates_token_count,
                "total_tokens": usage.total_token_count if hasattr(usage, 'total_token_count') else None
            }
        
        return {
            "text": response.text,
            "model": self.model_flash,
            "tools": tools_used,
            "urls": urls_processed,
            "token_usage": token_usage,
            "metadata": {"processing_type": "flash_rapid_analysis"}
        }
    
    async def _process_with_pro(self, request: ChatRequest) -> Dict[str, Any]:
        """Procesa solicitud con Gemini Pro"""
        print(f"ðŸ§  [Pro]: AnÃ¡lisis profundo")
        
        # Preparar prompt
        prompt_completo = self.pro_system_prompt + "\\n\\nConsulta del usuario: " + request.message
        
        # Agregar contenido del archivo si estÃ¡ presente
        if request.file_content:
            prompt_completo += f"\\n\\nContenido del documento para anÃ¡lisis:\\n{request.file_content}"
        
        # Ejecutar Pro sin herramientas web
        response = self.client.models.generate_content(
            model=self.model_pro,
            contents=prompt_completo
        )
        
        # InformaciÃ³n de tokens
        token_usage = None
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            usage = response.usage_metadata
            token_usage = {
                "input_tokens": usage.prompt_token_count,
                "output_tokens": usage.candidates_token_count,
                "total_tokens": usage.total_token_count if hasattr(usage, 'total_token_count') else None
            }
        
        return {
            "text": response.text,
            "model": self.model_pro,
            "tools": [],  # Pro no usa herramientas web
            "urls": [],
            "token_usage": token_usage,
            "metadata": {"processing_type": "pro_deep_analysis"}
        }
    
    def get_agent_status(self) -> Dict[str, Any]:
        """Retorna el estado del agente"""
        return {
            "status": "active",
            "models_available": [self.model_flash, self.model_pro],
            "active_sessions": len(self.sessions),
            "version": "3.0",
            "capabilities": [
                "financial_analysis",
                "web_search",
                "url_analysis", 
                "document_analysis",
                "real_time_data"
            ]
        }
